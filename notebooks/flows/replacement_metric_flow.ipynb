{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacement Metric Flow (to work around the lack of JAAD ground truth)\n",
    "======================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to get a model that can be used to \"fix\" the 2D poses of the JAAD dataset. General idea:\n",
    "1. JAAD does not have the ground truth, only 2D poses that are extracted from the images using OpenPose which are not very accurate.\n",
    "2. We have datasets containing the 3D poses in various formats, namely SMPL (we use CMU and HumanEva from AMASS) and CARLA skeleton (we use data recorded from the simulator, denoted as CarlaRec dataset). We can project the 3D poses (joints positions) to 2D poses using the camera parameters and get a \"good\" ground truth.\n",
    "3. We need to train a model using \"good\" datasets and then run the inference on JAAD and save the results.\n",
    "4. Then, we need to train another model (but with same architecture), this time using only the results from the inference on JAAD as the input and then run the inference on \"good\" datasets.\n",
    "5. Finally, we can compare the results from the second model with ground truth from \"good\" datasets and get a metric. If the model trained on \"fixed\" JAAD gets satisfactory results, we can assume that JAAD was correctly fixed and our model is good at generalizing/fixing the 2D poses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a model to use\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LinearAE2D'\n",
    "model_args = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and setup some common imports/data/training args as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pedestrians_video_2_carla.modeling import main\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_args = [\n",
    "    \"--flow=autoencoder\",\n",
    "    \"--batch_size=256\",\n",
    "    \"--input_nodes=CARLA_SKELETON\",\n",
    "    \"--output_nodes=CARLA_SKELETON\",\n",
    "    \"--loss_modes\",\n",
    "    \"loc_2d\",\n",
    "    \"--check_val_every_n_epoch=1\",\n",
    "    \"--renderers\",\n",
    "    \"none\",\n",
    "    \"--gpus=1\",\n",
    "    \"--clip_length=15\",\n",
    "    \"--clip_offset=15\",\n",
    "    \"--prefer_tensorboard\",\n",
    "    \"--mask_missing_joints=true\",\n",
    "    \"--disable_lr_scheduler\",\n",
    "    \"--num_workers=4\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model using CarlaRec + AMASS\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one_train = [\n",
    "    *common_args,\n",
    "    \"--data_module_name=JAADCarlaRecAMASS\",\n",
    "    \"--missing_point_probability=0.1\",\n",
    "    \"--noise=gaussian\",\n",
    "    \"--noise_param=1.0\",\n",
    "    \"--train_proportions\",\n",
    "    \"0\",\n",
    "    \"0.5\",\n",
    "    \"0.5\",\n",
    "    \"--val_proportions\",\n",
    "    \"0\",\n",
    "    \"-1\",\n",
    "    \"-1\",\n",
    "    \"--limit_val_batches=10\",\n",
    "    f\"--seed=1\",\n",
    "    #\n",
    "    \"--max_epochs=100\",\n",
    "    #\n",
    "    f\"--movements_model_name={model_name}\",\n",
    "    *model_args,\n",
    "]\n",
    "model_one_log_dir = main(model_one_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the trained model an run the inference on JAAD\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = glob.glob(os.path.join(model_one_log_dir, 'checkpoints', '*.*')).pop()\n",
    "model_one_predict = [\n",
    "    *common_args,\n",
    "    \"--mode=predict\",\n",
    "    \"--data_module_name=JAADOpenPose\",\n",
    "    \"--predict_sets\",\n",
    "    \"train\",\n",
    "    \"val\",\n",
    "    f\"--ckpt_path={model_checkpoint}\",\n",
    "    f\"--seed=2\",\n",
    "    f\"--movements_model_name={model_name}\",\n",
    "    *model_args,\n",
    "]\n",
    "main(model_one_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on the captured data\n",
    "--------------------------------\n",
    "\n",
    "The captured data is from JAAD, but it is in CARLA_SKELETON format. Therefore, we need to remember to force the Dataset to use CARLA_SKELETON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_one_name = os.path.basename(model_one_log_dir)\n",
    "jaad_subsets_dir = f'/outputs/JAADOpenPoseDataModulePredictions/subsets/598268da7cf7978df3eed284e07970c5/{run_one_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_two_train = [\n",
    "    *common_args,\n",
    "    \"--data_module_name=JAADOpenPose\",\n",
    "    \"--data_nodes=CARLA_SKELETON\",\n",
    "    \"--missing_point_probability=0.1\",\n",
    "    \"--noise=gaussian\",\n",
    "    \"--noise_param=1.0\",\n",
    "    f\"--seed=3\",\n",
    "    #\n",
    "    \"--max_epochs=100\",\n",
    "    #\n",
    "    f\"--movements_model_name={model_name}\",\n",
    "    f\"--subsets_dir={jaad_subsets_dir}\",\n",
    "    *model_args,\n",
    "]\n",
    "model_two_log_dir = main(model_two_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the trained model and run the inference on CarlaRec + AMASS\n",
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_two_checkpoint = glob.glob(os.path.join(model_two_log_dir, 'checkpoints', '*.*')).pop()\n",
    "model_two_predict_a = [\n",
    "    *common_args,\n",
    "    \"--mode=predict\",\n",
    "    \"--data_module_name=CarlaRecorded\",\n",
    "    \"--predict_sets\",\n",
    "    \"train\",\n",
    "    \"val\",\n",
    "    f\"--ckpt_path={model_two_checkpoint}\",\n",
    "    f\"--seed=4\",\n",
    "    f\"--movements_model_name={model_name}\",\n",
    "    *model_args,\n",
    "]\n",
    "model_two_predict_b = [\n",
    "    *common_args,\n",
    "    \"--mode=predict\",\n",
    "    \"--data_module_name=AMASS\",\n",
    "    \"--predict_sets\",\n",
    "    \"train\",\n",
    "    \"val\",\n",
    "    f\"--ckpt_path={model_two_checkpoint}\",\n",
    "    f\"--seed=5\",\n",
    "    f\"--movements_model_name={model_name}\",\n",
    "    *model_args,\n",
    "]\n",
    "\n",
    "main(model_two_predict_a)\n",
    "main(model_two_predict_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the target metrics\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_two_name = os.path.basename(model_two_log_dir)\n",
    "gt_carla_rec_subsets_dir = '/outputs/CarlaRecordedDataModule/subsets/7db72382d7a13dc69f8d4919228ea591'\n",
    "gt_amass_subsets_dir = '/outputs/AMASSDataModule/subsets/136b27b5869bd98ec133f22e327f6ec4'\n",
    "pred_carla_rec_subsets_dir = f'/outputs/CarlaRecordedDataModulePredictions/subsets/7db72382d7a13dc69f8d4919228ea591/{run_two_name}'\n",
    "pred_amass_subsets_dir = f'/outputs/AMASSDataModulePredictions/subsets/136b27b5869bd98ec133f22e327f6ec4/{run_two_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pedestrians_video_2_carla.transforms.normalization import Normalizer\n",
    "from pedestrians_video_2_carla.transforms.hips_neck_bbox_fallback import HipsNeckBBoxFallbackExtractor\n",
    "from pedestrians_video_2_carla.data.carla.skeleton import CARLA_SKELETON\n",
    "from pedestrians_video_2_carla.data.carla.carla_recorded_dataset import CarlaRecordedDataset\n",
    "from pedestrians_video_2_carla.data.smpl.skeleton import SMPL_SKELETON\n",
    "from pedestrians_video_2_carla.data.smpl.smpl_dataset import SMPLDataset\n",
    "\n",
    "common_kwargs = {\n",
    "    'input_nodes': CARLA_SKELETON,\n",
    "    'skip_metadata': True,\n",
    "    'transform': Normalizer(HipsNeckBBoxFallbackExtractor(CARLA_SKELETON))\n",
    "}\n",
    "\n",
    "gt_carla_rec = CarlaRecordedDataset(\n",
    "    set_filepath=os.path.join(gt_carla_rec_subsets_dir, 'val.hdf5'),\n",
    "    data_nodes=CARLA_SKELETON,\n",
    "    **common_kwargs\n",
    ")\n",
    "pred_carla_rec = CarlaRecordedDataset(\n",
    "    set_filepath=os.path.join(pred_carla_rec_subsets_dir, 'val.hdf5'),\n",
    "    data_nodes=CARLA_SKELETON,\n",
    "    **common_kwargs\n",
    ")\n",
    "\n",
    "gt_amass = SMPLDataset(\n",
    "    set_filepath=os.path.join(gt_amass_subsets_dir, 'val.hdf5'),\n",
    "    data_nodes=SMPL_SKELETON,\n",
    "    **{\n",
    "        **common_kwargs,\n",
    "        'transform': Normalizer(HipsNeckBBoxFallbackExtractor(SMPL_SKELETON))\n",
    "    }\n",
    ")\n",
    "pred_amass = SMPLDataset(\n",
    "    set_filepath=os.path.join(pred_amass_subsets_dir, 'val.hdf5'),\n",
    "    data_nodes=CARLA_SKELETON,\n",
    "    **common_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pedestrians_video_2_carla.transforms.hips_neck import HipsNeckExtractor\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics import MeanSquaredError\n",
    "from pedestrians_video_2_carla.metrics.multiinput_wrapper import MultiinputWrapper\n",
    "from pedestrians_video_2_carla.metrics.pck import PCK\n",
    "\n",
    "outputs_key = 'projection_2d_transformed'\n",
    "\n",
    "def get_normalization_tensor(x):\n",
    "    return HipsNeckExtractor(\n",
    "        input_nodes=CARLA_SKELETON\n",
    "    ).get_shift_scale(x)[1]\n",
    "\n",
    "metrics_collection = MetricCollection({\n",
    "    'MSE': MultiinputWrapper(\n",
    "        MeanSquaredError(dist_sync_on_step=True),\n",
    "        outputs_key, outputs_key,\n",
    "        input_nodes=CARLA_SKELETON,\n",
    "        output_nodes=CARLA_SKELETON,\n",
    "        mask_missing_joints=True,\n",
    "    ),\n",
    "    'PCKhn@01': PCK(\n",
    "        dist_sync_on_step=True,\n",
    "        input_nodes=CARLA_SKELETON,\n",
    "        output_nodes=CARLA_SKELETON,\n",
    "        mask_missing_joints=True,\n",
    "        key=outputs_key,\n",
    "        threshold=0.1,\n",
    "        get_normalization_tensor=get_normalization_tensor,\n",
    "    ),\n",
    "    'PCK@005': PCK(\n",
    "        dist_sync_on_step=True,\n",
    "        input_nodes=CARLA_SKELETON,\n",
    "        output_nodes=CARLA_SKELETON,\n",
    "        mask_missing_joints=True,\n",
    "        key=outputs_key,\n",
    "        threshold=0.05,\n",
    "        get_normalization_tensor=None,  # standard bbox normalization\n",
    "    ),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for gt_item, pred_item in tqdm(\n",
    "    zip(itertools.chain(gt_carla_rec, gt_amass), itertools.chain(pred_carla_rec, pred_amass)),\n",
    "    total=len(gt_carla_rec) + len(gt_amass)\n",
    "):\n",
    "    metrics_collection.update(gt_item[1], pred_item[1])\n",
    "\n",
    "results = metrics_collection.compute()\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "006d5deb8e6cdcd4312641bdf15f3bc20f0769a7305d81173599a7b40f33b4a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
